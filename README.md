# Random Forest for Avalanche Prediction
A wide variety of conditions directly affect the formation of different avalanche conditions such as wet, slab, and loose powder avalanches. Similar to weather forecasts, avalanche forecasts are issued daily during the winter months by several avalanche centers. Avalanche forecasts are generated by analyzing weather patterns and field reports conducted by snow science experts. These field reports involve digging deep snow pits , determining the state of the snowpack, and exposing the existence of weak layers. There are a limited number of people who are qualified to submit these field observations as the snow science field is still fairly new and is quite a labor-intensive task. Many areas lack the presence of these experts to make observation, so there is an opportunity to implement machine learning methods to facilitate and improve forecasting. <br>

I developed a codebase that constructs a random forest classification model from scratch in Go.The random forest is then used to predict avalanche problems from weather data. This model is based on the use of decision trees, which split the data into smaller and smaller subsets. The internal nodes of the decision tree represent a “test” on a certain independent variable (feature), either continuous or categorical, resulting in two branches. Two daughter nodes are created on each branch, which each storing a partition of the data split on the feature. The leaf nodes of the tree represent the dependent variable (target), in this case the specific avalanche problem likelihood, and represent only one category. The paths from the root to the leaves represent the various sets of classification rules that lead to prediction. <br>

A single decision tree is built with a greedy approach through the use of the Gini Index, which calculates the probability of the target being incorrectly classified when selected randomly from a given set. A Gini Index of 0 would indicate that all of the data points belong to a single category, while a value of 1 would suggest random distribution across all of the categories. This drives the decision towards determining which feature, and what value of that feature, minimizes the Gini Index. In order to take into account the number of data points in each partitioned data set, the Gini Index of each daughter node is weighted with the proportion of the samples in the split data set to the number of samples in the parent data set. Once the best split feature has been identified, the data set is partitioned based on the split, two daughter nodes are created, and the splitting continues recursively on each daughter node. Splitting is terminated on several conditions: if the max depth of the tree has been reached at the current node, if a split on the current node would drop below the minimum number of samples required on each leaf node, or if the current node of interest is a pure classifier node (leaf node). <br>

Single decision trees are prone to overfitting and high variance, whereas a random forest model makes use of multiple decision trees to mitigate the consequences of a single tree. Through a process called bootstrapping, training data required for building a single decision tree is randomly selected with replacement from the original training set. Another way that randomness is incorporated in the model is by selecting a different random subset of features that each tree uses to determine the best split. These methods rely on the idea that a large number of uncorrelated errors will average out to zero, with the goal of reducing variance. Due to each tree being built from randomly selected data and features, it is unlikely that two trees are correlated. Once the forest of decision trees has been constructed, a prediction for an unseen data point can be generated by passing it through each tree. For a classification problem, the category with the majority vote (mode) determined from the output of each tree is used as the prediction.
